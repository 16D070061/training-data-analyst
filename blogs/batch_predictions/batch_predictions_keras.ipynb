{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passing through instance keys and features when using a keras model\n",
    "\n",
    "This notebook will show you how to modify a Keras model, either loaded from disk or newly created, to perform keyed predictions or forward features through with the prediction.\n",
    "\n",
    "## Topics Covered\n",
    "- Modify serving signature of existing model to accept and forward keys\n",
    "- Multiple serving signatures on one model\n",
    "- Online and batch predictions with Google Cloud AI Platform\n",
    "- Forward features in model definition\n",
    "- Forward features with serving signature\n",
    "\n",
    "#TODO:\n",
    " - loaded model or not loaded model\n",
    " - Model.predict() doesn't take signature\n",
    " - parameterize the bash cells a little more\n",
    " - add more conceptual explanation when finish out the blog itself\n",
    " - add link to actual blog\n",
    " \n",
    "Q's\n",
    " - worth including BQML?\n",
    " - model.predict() doesn't seem to respect Input() layer name?\n",
    " - model.predict() doesn't output keys for the prediction dictionary\n",
    " - feature forward model has both signatures, too much?\n",
    " - tool long in general?\n",
    " - should load a saved model instead? I sort of liked that they saw where the layers were named and then corresponding key in teh signature\n",
    " - model in a model still 'feels' cleaner even though not recommended? don't have to mess with tf.function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0-dlenv_tfe'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train a Fashion MNIST model\n",
    "\n",
    "We will use a straightforward keras use case with the fashion mnist dataset to demonstrate building a model and then adding support for keyed predictions.\n",
    "More here:\n",
    "https://colab.sandbox.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale down dataset\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 12s 194us/sample - loss: 0.5175 - accuracy: 0.8210\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9db7883090>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build and traing model\n",
    "\n",
    "from tensorflow.keras import Sequential, Input\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "model = Sequential([\n",
    "  Input(shape=(28,28), name=\"image\"),\n",
    "  Flatten(input_shape=(28, 28), name=\"flatten\"),\n",
    "  Dense(64, activation='relu', name=\"dense\"),\n",
    "  Dense(10, activation='softmax', name=\"preds\"),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Only training for 1 epoch, we are not worried about model performance\n",
    "model.fit(train_images, train_labels, epochs=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.4721153e-05, 2.2903264e-06, 9.4477455e-06, 5.1241836e-06,\n",
       "        6.8963200e-05, 1.8251964e-01, 5.0299277e-05, 6.8799429e-02,\n",
       "        1.5516685e-03, 7.4695832e-01]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create test_image\n",
    "test_image = np.expand_dims(test_images[0],0).astype('float32')\n",
    "model.predict(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SavedModel and serving signature\n",
    "\n",
    "Now save the model using tf.saved_model.save(). This will add a serving signature which we can then inspect. The serving signature indicates exactly which input names and types are expected, and what will be output by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    }
   ],
   "source": [
    "MODEL_EXPORT_PATH = './model/'\n",
    "tf.saved_model.save(model, MODEL_EXPORT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['image'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 28, 28)\n",
      "      name: serving_default_image:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['preds'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 10)\n",
      "      name: StatefulPartitionedCall:0\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --tag_set serve --signature_def serving_default --dir {MODEL_EXPORT_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_SignatureMap({'serving_default': <tensorflow.python.saved_model.load._WrapperFunction object at 0x7f9d9c0dba50>})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model from storage and inspect the object types\n",
    "loaded_model = tf.keras.models.load_model(MODEL_EXPORT_PATH)\n",
    "loaded_model.signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.saving.saved_model.load.Sequential at 0x7f9d9c050c90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth noting that original model did not have serving signature until we saved it and is a slightly different object type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x7f9dbabcc910>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'signatures'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1ebee1a4162e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Expect an Error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'signatures'"
     ]
    }
   ],
   "source": [
    "# Expect an Error\n",
    "model.signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard serving function\n",
    "\n",
    "We can actually get access to the inference_function of the loaded model and is it directly to perform predictions, similar to a Keras Model.predict() call. Note the name of the output Tensor matches the serving signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.saved_model.load._WrapperFunction object at 0x7f9d9c0dba50>\n"
     ]
    }
   ],
   "source": [
    "inference_function = loaded_model.signatures['serving_default']\n",
    "\n",
    "print(inference_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'preds': <tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[3.4721153e-05, 2.2903264e-06, 9.4477455e-06, 5.1241836e-06,\n",
      "        6.8963200e-05, 1.8251964e-01, 5.0299277e-05, 6.8799429e-02,\n",
      "        1.5516685e-03, 7.4695832e-01]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "result = inference_function(tf.constant(test_image))\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       "array([[3.4721153e-05, 2.2903264e-06, 9.4477455e-06, 5.1241836e-06,\n",
       "        6.8963200e-05, 1.8251964e-01, 5.0299277e-05, 6.8799429e-02,\n",
       "        1.5516685e-03, 7.4695832e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matches serving signature\n",
    "result['preds']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyed Serving Function\n",
    "\n",
    "Now we'll create a new serving function that accepts and outputs a unique instance key. We use the fact that a Keras Model(x) call actually runs a prediction. The training=False parameter is included only for clarity. Then we save the model as before but provide this function as our new serving signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[tf.TensorSpec([None], dtype=tf.string),tf.TensorSpec([None, 28, 28], dtype=tf.float32)])\n",
    "def keyed_prediction(key, image):\n",
    "    pred = loaded_model(image, training=False)\n",
    "    return {\n",
    "        'preds': pred,\n",
    "        'key': key\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./keyed_model/assets\n"
     ]
    }
   ],
   "source": [
    "# Resave model, but specify new serving signature\n",
    "KEYED_EXPORT_PATH = './keyed_model/'\n",
    "loaded_model.save(KEYED_EXPORT_PATH, signatures={'serving_default': keyed_prediction})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['image'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 28, 28)\n",
      "      name: serving_default_image:0\n",
      "  inputs['key'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1)\n",
      "      name: serving_default_key:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['key'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1)\n",
      "      name: StatefulPartitionedCall:0\n",
      "  outputs['preds'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 10)\n",
      "      name: StatefulPartitionedCall:1\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --tag_set serve --signature_def serving_default --dir {KEYED_EXPORT_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyed_model = tf.keras.models.load_model(KEYED_EXPORT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.4721153e-05, 2.2903264e-06, 9.4477455e-06, 5.1241836e-06,\n",
       "        6.8963200e-05, 1.8251964e-01, 5.0299277e-05, 6.8799429e-02,\n",
       "        1.5516685e-03, 7.4695832e-01]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: why does this not work?\n",
    "# It won't even accept the Input layer name, and instead requires the autogenerated name of the 'flatten' layer\n",
    "keyed_model.predict({\n",
    "    'flatten_input': test_image,\n",
    "    'key': tf.constant(\"unique_key\")}\n",
    ")\n",
    "# keyed_model.predict(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Signature Model\n",
    "\n",
    "Sometimes it is useful to leave both signatures in the model definition so the user can indicate if they are performing a keyed prediction or not. This can easily be done with the model.save() method as before.\n",
    "\n",
    "In general, your serving infrastructure will default to 'serving_default' unless otherwise specified in a prediction call. Google Cloud AI Platform online and batch prediction support multiple signatures, as does [TFServing](https://www.tensorflow.org/tfx/serving/api_rest#request_format_2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./dual_signature_model/assets\n"
     ]
    }
   ],
   "source": [
    "# Using inference_function from earlier\n",
    "DUAL_SIGNATURE_EXPORT_PATH = './dual_signature_model/'\n",
    "loaded_model.save(DUAL_SIGNATURE_EXPORT_PATH, signatures={'serving_default': keyed_prediction,\n",
    "                                                  'unkeyed_signature': inference_function})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel MetaGraphDef contains SignatureDefs with the following keys:\n",
      "SignatureDef key: \"__saved_model_init_op\"\n",
      "SignatureDef key: \"serving_default\"\n"
     ]
    }
   ],
   "source": [
    "# Examine the multiple signatures\n",
    "!saved_model_cli show --tag_set serve --dir {KEYED_EXPORT_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['image'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 28, 28)\n",
      "      name: serving_default_image:0\n",
      "  inputs['key'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1)\n",
      "      name: serving_default_key:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['key'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1)\n",
      "      name: StatefulPartitionedCall:0\n",
      "  outputs['preds'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 10)\n",
      "      name: StatefulPartitionedCall:1\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "# Default signature\n",
    "!saved_model_cli show --tag_set serve --signature_def serving_default --dir {KEYED_EXPORT_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "Method name is: \n"
     ]
    }
   ],
   "source": [
    "# Alternative unkeyed signature\n",
    "!saved_model_cli show --tag_set serve --signature_def unkeyed_signature --dir {KEYED_EXPORT_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model and perform predictions\n",
    "\n",
    "Now we'll deploy the model to AI Platform serving and perform both online and batch keyed predictions. Deployment will take 2-3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fashion_mnist v1\n",
      "Model fashion_mnist already exists\n",
      "v1 gs://dhodun1/c89caeaabecb8a1820b7163a264899db920fceab2404bd34ea6f78458eb44ffa/ READY\n",
      "Deleting version v1\n",
      "Creating version v1 from ./dual_signature_model/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using endpoint [https://ml.googleapis.com/]\n",
      "WARNING: Using endpoint [https://ml.googleapis.com/]\n",
      "WARNING: Using endpoint [https://ml.googleapis.com/]\n",
      "This will delete version [v1]...\n",
      "\n",
      "Do you want to continue (Y/n)?  \n",
      "Deleting version [v1]......\n",
      "..............................................................................................................................................................done.\n",
      "WARNING: Using endpoint [https://ml.googleapis.com/]\n",
      "Creating version (this might take a few minutes)......\n",
      "...........................................................................................................................................................................................................................................................................................................................done.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "MODEL_LOCATION='./dual_signature_model/'\n",
    "MODEL_NAME=fashion_mnist\n",
    "MODEL_VERSION=v1\n",
    "\n",
    "TFVERSION=2.1\n",
    "REGION=us-central1\n",
    "BUCKET=dhodun1\n",
    "\n",
    "# create the model if it doesn't already exist\n",
    "modelname=$(gcloud ai-platform models list | grep -w \"$MODEL_NAME\")\n",
    "echo $modelname\n",
    "if [ -z \"$modelname\" ]; then\n",
    "   echo \"Creating model $MODEL_NAME\"\n",
    "   gcloud ai-platform models create ${MODEL_NAME} --regions $REGION\n",
    "else\n",
    "   echo \"Model $MODEL_NAME already exists\"\n",
    "fi\n",
    "\n",
    "# delete the model version if it already exists\n",
    "modelver=$(gcloud ai-platform versions list --model \"$MODEL_NAME\" | grep -w \"$MODEL_VERSION\")\n",
    "echo $modelver\n",
    "if [ \"$modelver\" ]; then\n",
    "   echo \"Deleting version $MODEL_VERSION\"\n",
    "   yes | gcloud ai-platform versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "   sleep 10\n",
    "fi\n",
    "\n",
    "\n",
    "echo \"Creating version $MODEL_VERSION from $MODEL_LOCATION\"\n",
    "gcloud ai-platform versions create ${MODEL_VERSION} \\\n",
    "       --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --staging-bucket gs://${BUCKET} \\\n",
    "       --runtime-version $TFVERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: There a way to paramterize %%writefile?\n",
    "# Create keyed test_image file\n",
    "\n",
    "with open(\"keyed_input.json\", \"w\") as file:\n",
    "    print(f'{{\"image\": {test_image.tolist()}, \"key\": \"hi\"}}', file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Using endpoint [https://ml.googleapis.com/]\n",
      "KEY  PREDS\n",
      "hi   [3.472115349723026e-05, 2.2903222998138517e-06, 9.447747288504615e-06, 5.124184099258855e-06, 6.896320701343939e-05, 0.1825195848941803, 5.029932799516246e-05, 0.0687994509935379, 0.0015516685089096427, 0.7469583749771118]\n"
     ]
    }
   ],
   "source": [
    "# Single online keyed prediction, --signature-name is not required since we're hitting the default but shown for clarity\n",
    "\n",
    "!gcloud ai-platform predict --model fashion_mnist --json-instances keyed_input.json --version v1 --signature-name serving_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unkeyed test_image file\n",
    "\n",
    "with open(\"unkeyed_input.json\", \"w\") as file:\n",
    "    print(f'{{\"image\": {test_image.tolist()}}}', file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Using endpoint [https://ml.googleapis.com/]\n",
      "PREDS\n",
      "[3.472115349723026e-05, 2.2903222998138517e-06, 9.447747288504615e-06, 5.124184099258855e-06, 6.896320701343939e-05, 0.1825195848941803, 5.029932799516246e-05, 0.0687994509935379, 0.0015516685089096427, 0.7469583749771118]\n"
     ]
    }
   ],
   "source": [
    "# Single online unkeyed prediction using alternative serving signature\n",
    "\n",
    "!gcloud ai-platform predict --model fashion_mnist --json-instances unkeyed_input.json --version v1 --signature-name unkeyed_signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Predictions\n",
    "\n",
    "Now we'll create multiple keyed prediction files and create a job to perform these predictions in a scalable, distributed manner. The keys will be retained so the results can be stored and associated with the initial inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data files:\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "DATA_DIR = './batch_data'\n",
    "shutil.rmtree(DATA_DIR, ignore_errors=True)\n",
    "os.makedirs(DATA_DIR)\n",
    "\n",
    "# Create 10 files with 10 images each\n",
    "for i in range(10):\n",
    "    with open(f'{DATA_DIR}/keyed_batch_{i}.json', \"w\") as file:\n",
    "        for z in range(10):\n",
    "            key = f'key_{i}_{z}'\n",
    "            print(f'{{\"image\": {test_images[z].tolist()}, \"key\": \"{key}\"}}', file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://./batch_data/keyed_batch_1.json [Content-Type=application/json]...\n",
      "Copying file://./batch_data/keyed_batch_9.json [Content-Type=application/json]...\n",
      "Copying file://./batch_data/keyed_batch_8.json [Content-Type=application/json]...\n",
      "Copying file://./batch_data/keyed_batch_3.json [Content-Type=application/json]...\n",
      "Copying file://./batch_data/keyed_batch_5.json [Content-Type=application/json]...\n",
      "Copying file://./batch_data/keyed_batch_6.json [Content-Type=application/json]...\n",
      "Copying file://./batch_data/keyed_batch_7.json [Content-Type=application/json]...\n",
      "Copying file://./batch_data/keyed_batch_2.json [Content-Type=application/json]...\n",
      "Copying file://./batch_data/keyed_batch_4.json [Content-Type=application/json]...\n",
      "Copying file://./batch_data/keyed_batch_0.json [Content-Type=application/json]...\n",
      "/ [10/10 files][870.0 KiB/870.0 KiB] 100% Done                                  \n",
      "Operation completed over 10 objects/870.0 KiB.                                   \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "BUCKET=dhodun1\n",
    "gsutil -m cp -r ./batch_data gs://$BUCKET/temp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This following batch prediction job took me 8-10 minutes, most of the time spent in infrastructure spin up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: fashion_mnist_batch_predict_20200611_162203\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [fashion_mnist_batch_predict_20200611_162203] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe fashion_mnist_batch_predict_20200611_162203\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs fashion_mnist_batch_predict_20200611_162203\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "DATA_FORMAT=\"text\" # JSON data format\n",
    "INPUT_PATHS='gs://dhodun1/temp/batch_data/*'\n",
    "OUTPUT_PATH='gs://dhodun1/temp/batch_predictions'\n",
    "MODEL_NAME='fashion_mnist'\n",
    "VERSION_NAME='v1'\n",
    "REGION='us-central1'\n",
    "now=$(date +\"%Y%m%d_%H%M%S\")\n",
    "JOB_NAME=\"fashion_mnist_batch_predict_$now\"\n",
    "LABELS=\"team=engineering,phase=test,owner=drew\"\n",
    "SIGNATURE_NAME=\"serving_default\"\n",
    "\n",
    "gcloud ai-platform jobs submit prediction $JOB_NAME \\\n",
    "    --model $MODEL_NAME \\\n",
    "    --version $VERSION_NAME \\\n",
    "    --input-paths $INPUT_PATHS \\\n",
    "    --output-path $OUTPUT_PATH \\\n",
    "    --region $REGION \\\n",
    "    --data-format $DATA_FORMAT \\\n",
    "    --labels $LABELS \\\n",
    "    --signature-name $SIGNATURE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can stream the logs, this cell will block until the job completes.\n",
    "# Copy and paste from the previous cell's output based to grab your job name\n",
    "\n",
    "# gcloud ai-platform jobs stream-logs fashion_mnist_batch_predict_20200611_151356"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://dhodun1/temp/batch_predictions/prediction.errors_stats-00000-of-00001\n",
      "gs://dhodun1/temp/batch_predictions/prediction.results-00000-of-00001\n",
      "gs://dhodun1/temp/batch_predictions/prediction.results-00000-of-00010\n",
      "gs://dhodun1/temp/batch_predictions/prediction.results-00001-of-00010\n",
      "gs://dhodun1/temp/batch_predictions/prediction.results-00002-of-00010\n",
      "gs://dhodun1/temp/batch_predictions/prediction.results-00003-of-00010\n",
      "gs://dhodun1/temp/batch_predictions/prediction.results-00004-of-00010\n",
      "gs://dhodun1/temp/batch_predictions/prediction.results-00005-of-00010\n",
      "gs://dhodun1/temp/batch_predictions/prediction.results-00006-of-00010\n",
      "gs://dhodun1/temp/batch_predictions/prediction.results-00007-of-00010\n",
      "gs://dhodun1/temp/batch_predictions/prediction.results-00008-of-00010\n",
      "gs://dhodun1/temp/batch_predictions/prediction.results-00009-of-00010\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://dhodun1/temp/batch_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"preds\": [4.013196667074226e-05, 7.403023118968122e-06, 3.707501718963613e-06, 1.3309241921888315e-06, 2.7610798497335054e-05, 0.12554031610488892, 4.4301763409748673e-05, 0.2407132089138031, 0.005084679462015629, 0.6285373568534851], \"key\": \"key_8_0\"}\n",
      "{\"preds\": [0.0017545941518619657, 2.4121516162267653e-06, 0.8372505307197571, 0.0012869687052443624, 0.0498567670583725, 1.833312808230403e-07, 0.10880657285451889, 2.6277988118827977e-10, 0.0010421136394143105, 5.360926813580136e-09], \"key\": \"key_8_1\"}\n",
      "{\"preds\": [2.7995271011604927e-05, 0.9999473094940186, 1.4881048855386325e-06, 1.5070620065671392e-05, 7.655904482817277e-06, 4.014826515685854e-08, 1.3820189792568272e-07, 1.0850141229923338e-08, 1.973001246824424e-07, 4.457038116356671e-08], \"key\": \"key_8_2\"}\n",
      "{\"preds\": [1.7342121282126755e-05, 0.9997573494911194, 1.1361087672412395e-05, 0.00018262902449350804, 2.446685357426759e-05, 3.6486651424638694e-06, 1.1849277825604076e-06, 1.1300596014507391e-07, 1.1253807770117419e-06, 7.832731512280589e-07], \"key\": \"key_8_3\"}\n",
      "{\"preds\": [0.16231335699558258, 0.00036586119676940143, 0.0758107379078865, 0.0205352995544672, 0.07288094609975815, 3.066795034101233e-05, 0.6480918526649475, 3.2602572446194245e-06, 0.019952911883592606, 1.509881349193165e-05], \"key\": \"key_8_4\"}\n",
      "{\"preds\": [0.003565847175195813, 0.9938931465148926, 0.00016754464013502002, 0.0007457845495082438, 0.0015098609728738666, 1.2779814824170899e-06, 8.34084494272247e-05, 1.9685689949255902e-06, 2.816929918481037e-05, 2.9632712994498434e-06], \"key\": \"key_8_5\"}\n",
      "{\"preds\": [0.020846690982580185, 0.000536054780241102, 0.02098439261317253, 0.006189421750605106, 0.7985213398933411, 0.003130595898255706, 0.13984042406082153, 0.00011236956197535619, 0.009790771640837193, 4.8011785111157224e-05], \"key\": \"key_8_6\"}\n",
      "{\"preds\": [0.0016981314402073622, 0.00025639074738137424, 0.014810278080403805, 0.0010117498459294438, 0.2255433052778244, 0.0006777368253096938, 0.7501516342163086, 1.7973228523260332e-06, 0.005818618927150965, 3.0330924346344545e-05], \"key\": \"key_8_7\"}\n",
      "{\"preds\": [0.008030816912651062, 0.00608327891677618, 0.00340915285050869, 0.0008357667247764766, 0.0018228712724521756, 0.8642773628234863, 0.012844962999224663, 0.08159439265727997, 0.019452538341283798, 0.0016487770481035113], \"key\": \"key_8_8\"}\n",
      "{\"preds\": [3.7274163332767785e-05, 3.2053529139375314e-05, 3.205672328476794e-06, 9.218909440278367e-07, 3.622735903263674e-06, 0.006455224007368088, 6.77045409247512e-06, 0.9906542897224426, 0.001930785016156733, 0.0008757943287491798], \"key\": \"key_8_9\"}\n"
     ]
    }
   ],
   "source": [
    "# View predictions with keys\n",
    "!gsutil cat 'gs://dhodun1/temp/batch_predictions/prediction.results-00000-of-00010'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigQuery ML Batch Predictions\n",
    "\n",
    "\n",
    "#TODO: change shape of model so it works? Doesn't support (28,28 input shape)\n",
    "\n",
    "If your source data exists in BigQuery, an alternative is to load your Tensorflow model into [BQML](https://cloud.google.com/bigquery-ml/docs/making-predictions-with-imported-tensorflow-models#overview) and perform predictions directly in the data warehouse. This also removes the need to produced keyed predictions at all.\n",
    "\n",
    "[Not all datatypes are supported](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-tensorflow#inputs) and some use cases are less conducive to this, in our case, an image model.\n",
    "\n",
    "Loading the model would look something like this:\n",
    "\n",
    "```\n",
    "CREATE OR REPLACE MODEL my_dataset.fashion_mnist\n",
    "OPTIONS (MODEL_TYPE='TENSORFLOW',\n",
    "    MODEL_PATH='gs://dhodun1/temp/model/*')\n",
    "```\n",
    "\n",
    "And performing batch predictions:\n",
    "\n",
    "#TODO: are other features forwarded along?\n",
    "\n",
    "```\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  ML.PREDICT(MODEL `my_dataset.fashion_mnist`,\n",
    "    (\n",
    "    SELECT\n",
    "      image,\n",
    "      \n",
    "    FROM\n",
    "      `my_datset.images`))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Forward Models\n",
    "\n",
    "There are also times where it's desirable to forward some or all of the input features along with the output. This can be achieved in a very similar manner as adding keyed outputs to our model.\n",
    "\n",
    "Note that this will be a little trickier to grab a subset of features if you are feeding all of your input features as a single Input() layer in the Keras model. This example takes multiple Inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a toy model using the Boston Housing dataset\n",
    "# https://www.kaggle.com/c/boston-housing\n",
    "# Prediction target is median value of homes in $1000's\n",
    "\n",
    "(train_data, train_targets), _ = keras.datasets.boston_housing.load_data()\n",
    "\n",
    "# Extract just two of the features for simplicity's sake\n",
    "train_tax_rate = train_data[:,10]\n",
    "train_rooms = train_data[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a toy model with multiple inputs\n",
    "# This time using the Keras functional API\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "\n",
    "tax_rate = Input(shape=(1,), dtype=tf.float32, name=\"tax_rate\")\n",
    "rooms = Input(shape=(1,), dtype=tf.float32, name=\"rooms\")\n",
    "\n",
    "x = tf.keras.layers.Concatenate()([tax_rate, rooms])\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "price = tf.keras.layers.Dense(1, activation=None, name=\"price\")(x)\n",
    "\n",
    "# Functional API model instead of Sequential\n",
    "model = Model(inputs=[tax_rate, rooms], outputs=[price])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 404 samples\n",
      "Epoch 1/10\n",
      "404/404 [==============================] - 0s 1ms/sample - loss: 537.3960 - accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "404/404 [==============================] - 0s 96us/sample - loss: 418.1529 - accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "404/404 [==============================] - 0s 94us/sample - loss: 320.9664 - accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "404/404 [==============================] - 0s 103us/sample - loss: 246.3120 - accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "404/404 [==============================] - 0s 90us/sample - loss: 190.5645 - accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "404/404 [==============================] - 0s 112us/sample - loss: 149.0178 - accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "404/404 [==============================] - 0s 104us/sample - loss: 125.7651 - accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "404/404 [==============================] - 0s 121us/sample - loss: 109.7265 - accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "404/404 [==============================] - 0s 100us/sample - loss: 102.1707 - accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "404/404 [==============================] - 0s 103us/sample - loss: 98.8275 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9d603f8110>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "model.fit([train_tax_rate, train_rooms], train_targets, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature forward and non feature forward predictions\n",
    "\n",
    "Using the Keras sequential API, we create another model with slightly different inputs and outputs, but retaining the weights of the existing model. Notice the predictions with and without feature forwarding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22.480536]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict({\n",
    "    'tax_rate': tf.convert_to_tensor([20.2]),\n",
    "    'rooms': tf.convert_to_tensor([6.2])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./boston_model/assets\n"
     ]
    }
   ],
   "source": [
    "BOSTON_EXPORT_PATH = './boston_model/'\n",
    "model.save(BOSTON_EXPORT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will retain weights from trained model but also forward out a feature\n",
    "forward_model = Model(inputs=[tax_rate, rooms], outputs=[price, tax_rate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[9.465022]], dtype=float32), array([[5.]], dtype=float32)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice we get both outputs now\n",
    "# TODO: why lack of keys on the prediction dictionary?\n",
    "forward_model.predict({\n",
    "    'tax_rate': tf.convert_to_tensor([5.0]),\n",
    "    'rooms': tf.convert_to_tensor([6.2])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./forward_model/assets\n"
     ]
    }
   ],
   "source": [
    "FORWARD_EXPORT_PATH = './forward_model/'\n",
    "forward_model.save(FORWARD_EXPORT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['rooms'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_rooms:0\n",
      "  inputs['tax_rate'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_tax_rate:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['price'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: StatefulPartitionedCall:0\n",
      "  outputs['tax_rate'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: StatefulPartitionedCall:1\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --tag_set serve --signature_def serving_default --dir {FORWARD_EXPORT_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forwarding by changing serving signature\n",
    "\n",
    "We could have employed the same method as before to also modify the serving signature and save out the model to achieve the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['rooms'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_rooms:0\n",
      "  inputs['tax_rate'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_tax_rate:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['price'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: StatefulPartitionedCall:0\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --tag_set serve --signature_def serving_default --dir {BOSTON_EXPORT_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Is there way to avoid having to create this? In the previous example we grabbed the funcion from a Loaded model\n",
    "@tf.function(input_signature=[tf.TensorSpec([None, 1], dtype=tf.float32), tf.TensorSpec([None, 1], dtype=tf.float32)])\n",
    "def standard_forward_prediction(tax_rate, rooms):\n",
    "    pred = model([tax_rate, rooms], training=False)\n",
    "    return {\n",
    "        'price': pred,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return out the feature of interest as well as the prediction\n",
    "@tf.function(input_signature=[tf.TensorSpec([None, 1], dtype=tf.float32), tf.TensorSpec([None, 1], dtype=tf.float32)])\n",
    "def feature_forward_prediction(tax_rate, rooms):\n",
    "    pred = model([tax_rate, rooms], training=False)\n",
    "    return {\n",
    "        'price': pred,\n",
    "        'tax_rate': tax_rate\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./dual_signature_forward_model/assets\n"
     ]
    }
   ],
   "source": [
    "# Save out the model with both signatures\n",
    "DUAL_SIGNATURE_FORWARD_PATH = './dual_signature_forward_model/'\n",
    "model.save(DUAL_SIGNATURE_FORWARD_PATH, signatures={'serving_default': standard_forward_prediction,\n",
    "                                   'feature_forward': feature_forward_prediction})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['rooms'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: feature_forward_rooms:0\n",
      "  inputs['tax_rate'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: feature_forward_tax_rate:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['price'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: StatefulPartitionedCall:0\n",
      "  outputs['tax_rate'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: StatefulPartitionedCall:1\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "# Inspect just the feature_forward signature, but we also have standard serving_default\n",
    "!saved_model_cli show --tag_set serve --signature_def feature_forward --dir {DUAL_SIGNATURE_FORWARD_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 Google Inc.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
