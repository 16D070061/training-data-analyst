{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured data prediction using Cloud ML Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates:\n",
    "\n",
    "1. Exploring a BigQuery dataset using JupyterLab\n",
    "2. Creating datasets for Machine Learning using Dataflow\n",
    "3. Creating a model using the feature columns and Keras API\n",
    "4. Training on Cloud AI Platform\n",
    "5. Deploying model\n",
    "6. Predicting with model\n",
    "\n",
    "Before starting the lab, upgrade packages that are required for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now you have to restart the kernel by selecting the \"Kernel\" -> \"Restart Kernel\" from the menu bar** to reflect the newly installed modules.\n",
    "\n",
    "After restarting the kernel, you can resume the code execution from the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "BUCKET = 'etsuj-baby-keras-ml'\n",
    "PROJECT = 'etsuj-baby-keras'\n",
    "REGION = 'us-central1'\n",
    "NOTEBOOK_DIR = '~/training-data-analyst/blogs/babyweight_keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['NOTEBOOK_DIR'] = NOTEBOOK_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! gsutil ls | grep -q gs://${BUCKET}/; then\n",
    "    gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Analysis and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring data\n",
    "\n",
    "The data is natality data (record of births in the US). My goal is to predict the baby's weight given a number of factors about the pregnancy and the baby's mother. Later, we will want to split the data into training and eval datasets. The hash of the year-month will be used for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"\n",
    "SELECT\n",
    "  weight_pounds,\n",
    "  is_male,\n",
    "  mother_age,\n",
    "  plurality,\n",
    "  gestation_weeks,\n",
    "  FARM_FINGERPRINT(CONCAT(CAST(YEAR AS STRING), CAST(month AS STRING))) AS hashmonth\n",
    "FROM\n",
    "  publicdata.samples.natality\n",
    "WHERE year > 2000\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "bq = bigquery.Client()\n",
    "\n",
    "query_job = bq.query(query + \" LIMIT 100\")\n",
    "df = query_job.to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a query to find the unique values for each of the columns and the count of those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distinct_values(column_name):\n",
    "    sql = \"\"\"\n",
    "SELECT\n",
    "  {0},\n",
    "  COUNT(1) AS num_babies,\n",
    "  AVG(weight_pounds) AS avg_wt\n",
    "FROM\n",
    "  publicdata.samples.natality\n",
    "WHERE\n",
    "  year > 2000\n",
    "GROUP BY\n",
    "  {0}\n",
    "    \"\"\".format(column_name)\n",
    "    return bq.query(sql).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_distinct_values('is_male')\n",
    "df.plot(x='is_male', y='num_babies', kind='bar')\n",
    "df.plot(x='is_male', y='avg_wt', kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_distinct_values('mother_age')\n",
    "df = df.sort_values('mother_age')\n",
    "df.plot(x='mother_age', y='num_babies')\n",
    "df.plot(x='mother_age', y='avg_wt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_distinct_values('plurality')\n",
    "df = df.sort_values('plurality')\n",
    "df.plot(x='plurality', y='num_babies', logy=True, kind='bar')\n",
    "df.plot(x='plurality', y='avg_wt', kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_distinct_values('gestation_weeks')\n",
    "df = df.sort_values('gestation_weeks')\n",
    "df.plot(x='gestation_weeks', y='num_babies', logy=True, kind='bar', color='royalblue')\n",
    "df.plot(x='gestation_weeks', y='avg_wt', kind='bar', color='royalblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these factors seem to play a part in the baby's weight. Male babies are heavier on average than female babies. Teenaged and older moms tend to have lower-weight babies. Twins, triplets, etc. are lower weight than single births. Preemies weigh in lower as do babies born to single moms. In addition, it is important to check whether you have enough data (number of babies) for each input value. Otherwise, the model prediction against input values that doesn't have enough data may not be reliable.\n",
    "\n",
    "In the rest of this notebook, we will use machine learning to combine all of these factors to come up with a prediction of a baby's weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a ML dataset using Dataflow\n",
    "I'm going to use Cloud Dataflow to read in the BigQuery data, do some preprocessing, and write it out as CSV files.\n",
    "\n",
    "Instead of using Beam/Dataflow, I had three other options:\n",
    "\n",
    "1. Use Cloud Dataprep to visually author a Dataflow pipeline. Cloud Dataprep also allows me to explore the data, so we could have avoided much of the handcoding of Python/Seaborn calls above as well!\n",
    "2. Read from BigQuery directly using TensorFlow.\n",
    "3. Use the BigQuery console (http://bigquery.cloud.google.com) to run a Query and save the result as a CSV file. For larger datasets, you may have to select the option to \"allow large results\" and save the result into a CSV file on Google Cloud Storage.\n",
    "\n",
    "However, in this case, I want to do some preprocessing. I want to modify the data such that we can simulate what is known if no ultrasound has been performed. If I didn't need preprocessing, I could have used the web console. Also, I prefer to script it out rather than run queries on the user interface. Therefore, I am using Cloud Dataflow for the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import datetime\n",
    "\n",
    "def to_csv(rowdict):\n",
    "    # pull columns from BQ and create a line\n",
    "    import hashlib\n",
    "    import copy\n",
    "    CSV_COLUMNS = 'weight_pounds,is_male,mother_age,plurality,gestation_weeks'.split(',')\n",
    "    \n",
    "    # create synthetic data where we assume that no ultrasound has been performed\n",
    "    # and so we don't know sex of the baby. Let's assume that we can tell the difference\n",
    "    # between single and multiple, but that the errors rates in determining exact number\n",
    "    # is difficult in the absence of an ultrasound.\n",
    "    no_ultrasound = copy.deepcopy(rowdict)\n",
    "    w_ultrasound = copy.deepcopy(rowdict)\n",
    "\n",
    "    no_ultrasound['is_male'] = 'Unknown'\n",
    "    if rowdict['plurality'] > 1:\n",
    "        no_ultrasound['plurality'] = 'Multiple(2+)'\n",
    "    else:\n",
    "        no_ultrasound['plurality'] = 'Single(1)'\n",
    "      \n",
    "    # Change the plurality column to strings\n",
    "    w_ultrasound['plurality'] = \\\n",
    "        ['Single(1)', 'Twins(2)', 'Triplets(3)', 'Quadruplets(4)', 'Quintuplets(5)'][rowdict['plurality']-1]\n",
    "    \n",
    "    # Write out two rows for each input row, one with ultrasound and one without\n",
    "    for result in [no_ultrasound, w_ultrasound]:\n",
    "        data = ','.join([str(result[k]) if k in result else 'None' for k in CSV_COLUMNS])\n",
    "        key = hashlib.sha224(data.encode('utf-8')).hexdigest()  # hash the columns to form a key\n",
    "        yield str('{},{}'.format(data, key))\n",
    "  \n",
    "def preprocess(in_test_mode):\n",
    "    job_name = 'preprocess-babyweight-features' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "    \n",
    "    if in_test_mode:\n",
    "        OUTPUT_DIR = './preproc'\n",
    "    else:\n",
    "        OUTPUT_DIR = 'gs://{0}/babyweight/preproc/'.format(BUCKET)\n",
    "    \n",
    "    options = {\n",
    "        'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "        'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "        'job_name': job_name,\n",
    "        'project': PROJECT,\n",
    "        'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "        'max_num_workers': 3,   # CHANGE THIS IF YOU HAVE MORE QUOTA\n",
    "        'no_save_main_session': True\n",
    "    }\n",
    "    opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "    if in_test_mode:\n",
    "        RUNNER = 'DirectRunner'\n",
    "    else:\n",
    "        RUNNER = 'DataflowRunner'\n",
    "    p = beam.Pipeline(RUNNER, options=opts)\n",
    "    query = \"\"\"\n",
    "SELECT\n",
    "  weight_pounds,\n",
    "  is_male,\n",
    "  mother_age,\n",
    "  plurality,\n",
    "  gestation_weeks,\n",
    "  FARM_FINGERPRINT(CONCAT(CAST(YEAR AS STRING), CAST(month AS STRING))) AS hashmonth\n",
    "FROM\n",
    "  publicdata.samples.natality\n",
    "WHERE year > 2000\n",
    "AND weight_pounds > 0\n",
    "AND mother_age > 0\n",
    "AND plurality > 0\n",
    "AND gestation_weeks > 0\n",
    "AND month > 0\n",
    "    \"\"\"\n",
    "  \n",
    "    if in_test_mode:\n",
    "        query = query + ' LIMIT 100' \n",
    "  \n",
    "    for step in ['train', 'eval']:\n",
    "        if step == 'train':\n",
    "            selquery = 'SELECT * FROM ({}) WHERE ABS(MOD(hashmonth, 4)) < 3'.format(query)\n",
    "        else:\n",
    "            selquery = 'SELECT * FROM ({}) WHERE ABS(MOD(hashmonth, 4)) = 3'.format(query)\n",
    "\n",
    "        (p \n",
    "         | '{}_read'.format(step) >> beam.io.Read(beam.io.BigQuerySource(query=selquery, use_standard_sql=True))\n",
    "         | '{}_csv'.format(step) >> beam.FlatMap(to_csv)\n",
    "         | '{}_out'.format(step) >> beam.io.Write(beam.io.WriteToText(os.path.join(OUTPUT_DIR, '{}.csv'.format(step))))\n",
    "        )\n",
    " \n",
    "    job = p.run()\n",
    "  \n",
    "preprocess(in_test_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may get a warning about access scopes. It's safe to ignore this.\n",
    "\n",
    "Note that after you launch this, the actual processing is happening on the Cloud. Go to the GCP web console to the Dataflow section and monitor the running job. You'll see a job that's running. It took about **30 minutes** for me.\n",
    "\n",
    "Once the job has completed, run the cell below to check the location of the are processed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/babyweight/preproc/*-00000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Developing a Machine Learning Model using TensorFlow and Cloud ML Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a TensorFlow model using the Estimator API\n",
    "\n",
    "First, write an read_dataset to create a generator object that reads the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS = 'weight_pounds,is_male,mother_age,plurality,gestation_weeks,key'.split(',')\n",
    "SELECT_COLUMNS = 'weight_pounds,is_male,mother_age,plurality,gestation_weeks'.split(',')\n",
    "LABEL_COLUMN = 'weight_pounds'\n",
    "\n",
    "def read_dataset(prefix, pattern, batch_size=512, eval=False):\n",
    "    # use prefix to create filename\n",
    "    filename = 'gs://{}/babyweight/preproc/{}*{}*'.format(BUCKET, prefix, pattern)\n",
    "    if eval:\n",
    "        dataset = tf.data.experimental.make_csv_dataset(\n",
    "            filename, header=False, batch_size=batch_size,\n",
    "            shuffle=False, num_epochs=1,\n",
    "            column_names=CSV_COLUMNS, label_name=LABEL_COLUMN,\n",
    "            select_columns=SELECT_COLUMNS\n",
    "        )\n",
    "    else:\n",
    "        dataset = tf.data.experimental.make_csv_dataset(\n",
    "            filename, header=False, batch_size=batch_size,\n",
    "            shuffle=True, num_epochs=None,\n",
    "            column_names=CSV_COLUMNS, label_name=LABEL_COLUMN,\n",
    "            select_columns=SELECT_COLUMNS\n",
    "        )\n",
    "    return dataset    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wide_deep():\n",
    "    # defin model inputs\n",
    "    inputs = {}\n",
    "    inputs['is_male'] = layers.Input(shape=(), name='is_male', dtype='string')\n",
    "    inputs['plurality'] = layers.Input(shape=(), name='plurality', dtype='string')\n",
    "    inputs['mother_age'] = layers.Input(shape=(), name='mother_age', dtype='float32')\n",
    "    inputs['gestation_weeks'] = layers.Input(shape=(), name='gestation_weeks', dtype='float32')\n",
    "    \n",
    "    # define column types\n",
    "    is_male = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        'is_male', ['True', 'False', 'Unknown'])\n",
    "    plurality = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        'plurality', ['Single(1)', 'Twins(2)', 'Triplets(3)', 'Quadruplets(4)', 'Quintuplets(5)', 'Multiple(2+)'])    \n",
    "    mother_age = tf.feature_column.numeric_column('mother_age')\n",
    "    gestation_weeks = tf.feature_column.numeric_column('gestation_weeks')\n",
    "\n",
    "    # discretize\n",
    "    age_buckets = tf.feature_column.bucketized_column(\n",
    "        mother_age, boundaries=np.arange(15, 45, 1).tolist())\n",
    "    gestation_buckets = tf.feature_column.bucketized_column(\n",
    "        gestation_weeks, boundaries=np.arange(17, 47, 1).tolist())\n",
    "\n",
    "    # sparse columns are wide \n",
    "    wide = [tf.feature_column.indicator_column(is_male),\n",
    "            tf.feature_column.indicator_column(plurality),\n",
    "            age_buckets,\n",
    "            gestation_buckets]\n",
    "    \n",
    "    # feature cross all the wide columns and embed into a lower dimension\n",
    "    crossed = tf.feature_column.crossed_column(\n",
    "        [is_male, plurality, age_buckets, gestation_buckets], hash_bucket_size=20000)\n",
    "    embed = tf.feature_column.embedding_column(crossed, 3)\n",
    "    \n",
    "    # continuous columns are deep\n",
    "    deep = [mother_age,\n",
    "            gestation_weeks,\n",
    "            embed]\n",
    "\n",
    "    return wide, deep, inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the wide and deep model using the Keras API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model():\n",
    "    wide, deep, inputs = get_wide_deep()\n",
    "    feature_layer_wide = layers.DenseFeatures(wide, name='wide_features')\n",
    "    feature_layer_deep = layers.DenseFeatures(deep, name='deep_features')\n",
    "\n",
    "    wide_model = feature_layer_wide(inputs)\n",
    "    \n",
    "    deep_model = layers.Dense(64, activation='relu', name='DNN_layer1')(feature_layer_deep(inputs))\n",
    "    deep_model = layers.Dense(32, activation='relu', name='DNN_layer2')(deep_model)\n",
    "\n",
    "    wide_deep_model = layers.Dense(1, name='weight')(layers.concatenate([wide_model, deep_model]))\n",
    "    model = models.Model(inputs=inputs, outputs=wide_deep_model)\n",
    "\n",
    "    # Compile Keras model\n",
    "    model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(lr=0.0001))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, train the model locally on the notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = create_keras_model()\n",
    "\n",
    "PATTERN = \"00000-of-\"  # process only one of the shards, for testing purposes\n",
    "BATCH_SIZE = 512\n",
    "NUM_TRAIN_EXAMPLES = 1000000\n",
    "NUM_EVAL_EXAMPLES = 10000\n",
    "NUM_EVALS = 10\n",
    "\n",
    "training_dataset = read_dataset('train', PATTERN, BATCH_SIZE)\n",
    "validation_dataset = read_dataset('eval', PATTERN, BATCH_SIZE).take(NUM_EVAL_EXAMPLES//BATCH_SIZE)\n",
    "\n",
    "history = keras_model.fit(\n",
    "    training_dataset,\n",
    "    steps_per_epoch=NUM_TRAIN_EXAMPLES//(BATCH_SIZE * NUM_EVALS),\n",
    "    epochs=NUM_EVALS,\n",
    "    validation_data=validation_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model=keras_model, to_file=\"dnn_model.png\", show_shapes=False, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can export the trainined model in the saved_model format with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "ts = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "export_path = \"gs://{}/babyweight/trained_model/export/{}\".format(BUCKET, ts)\n",
    "keras_model.save(export_path, save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the TensorFlow code working on a subset of the data (in the code above, I was reading only the 00000-of-x file), we can package the TensorFlow code up as a Python module and train it on Cloud AI Platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd $NOTEBOOK_DIR\n",
    "ls -l trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After moving the code to a package, make sure it works standalone. (Note the --num-train-example and --num-eval-examples lines so that I am not trying to boil the ocean on my laptop). Even then, this takes about **a minute** in which you won't see any output..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd $NOTEBOOK_DIR\n",
    "gcloud ai-platform local train \\\n",
    " --package-path trainer \\\n",
    " --module-name trainer.task \\\n",
    " --job-dir local-training-output \\\n",
    " -- \\\n",
    " --bucket etsuj-baby-keras-ml \\\n",
    " --num-train-examples 10000 \\\n",
    " --num-eval-examples 1000 \\\n",
    " --num-evals 4 \\\n",
    " --learning-rate 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the code works in standalone mode, you can run it on Cloud AI Platform. Because this is on the entire dataset, it will take a while. The training run took about **45 min** for me. You can monitor the job from the GCP console in the AI Platform section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "JOB_NAME=babyweight_$(date -u +%y%m%d_%H%M%S)\n",
    "JOB_DIR=gs://${BUCKET}/babyweight/trained_model\n",
    "\n",
    "cd $NOTEBOOK_DIR\n",
    "gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "  --job-dir $JOB_DIR \\\n",
    "  --package-path trainer/ \\\n",
    "  --module-name trainer.task \\\n",
    "  --region $REGION \\\n",
    "  --python-version 3.7 \\\n",
    "  --runtime-version 2.2 \\\n",
    "  --scale-tier basic-gpu \\\n",
    "  -- \\\n",
    "  --bucket etsuj-baby-keras-ml \\\n",
    "  --num-train-examples 40000000 \\\n",
    "  --num-eval-examples 50000 \\\n",
    "  --num-evals 100 \\\n",
    "  --learning-rate 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training logs are stored in the following location of the GCS bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/babyweight/trained_model/tensorboard/ | tail -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can launch the TensorBoard with the followind command on the Cloud Shell. Use the web preview with port 6006 to access it.\n",
    "\n",
    "```\n",
    "tensorboard --logdir [GCS path]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the trained model\n",
    "\n",
    "Deploying the trained model to act as a REST web service is a simple gcloud call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/babyweight/trained_model/export/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_NAME=\"babyweight\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/babyweight/trained_model/export/ | tail -1)\n",
    "echo \"Deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes\"\n",
    "#gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "#gcloud ml-engine models delete ${MODEL_NAME}\n",
    "gcloud ai-platform models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ai-platform versions create ${MODEL_VERSION} --model ${MODEL_NAME} \\\n",
    "  --origin ${MODEL_LOCATION} --runtime-version 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model to predict\n",
    "\n",
    "Send a JSON request to the endpoint of the service to make it predict a baby's weight ... I am going to try out how well the model would have predicted the weights of our two kids and a couple of variations while we are at it ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json\n",
    "\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "api = discovery.build('ml', 'v1', credentials=credentials, cache_discovery=False)\n",
    "\n",
    "request_data = {'instances':\n",
    "  [\n",
    "    {\n",
    "      'is_male': 'True',\n",
    "      'mother_age': 26.0,\n",
    "      'plurality': 'Single(1)',\n",
    "      'gestation_weeks': 39\n",
    "    },\n",
    "    {\n",
    "      'is_male': 'False',\n",
    "      'mother_age': 29.0,\n",
    "      'plurality': 'Single(1)',\n",
    "      'gestation_weeks': 38\n",
    "    },\n",
    "    {\n",
    "      'is_male': 'True',\n",
    "      'mother_age': 26.0,\n",
    "      'plurality': 'Triplets(3)',\n",
    "      'gestation_weeks': 39\n",
    "    },\n",
    "    {\n",
    "      'is_male': 'Unknown',\n",
    "      'mother_age': 29.0,\n",
    "      'plurality': 'Multiple(2+)',\n",
    "      'gestation_weeks': 38\n",
    "    },\n",
    "  ]\n",
    "}\n",
    "\n",
    "parent = 'projects/%s/models/%s/versions/%s' % (PROJECT, 'babyweight', 'v1')\n",
    "response = api.projects().predict(body=request_data, name=parent).execute()\n",
    "print(json.dumps(response, sort_keys = True, indent = 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran this, the four predictions for each of the requests in request_data above are 8.1, 7.6, 6.7, and 6.6 pounds. Yours may be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
