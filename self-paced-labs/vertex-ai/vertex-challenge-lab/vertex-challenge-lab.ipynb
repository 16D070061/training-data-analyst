{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f19fe218-8272-4a78-95dc-b45c7944d26d",
   "metadata": {},
   "source": [
    "# Building and deploying machine learning solutions with Vertex AI: Challenge Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4908fb9b-2048-48fc-a42c-2fdf76aea51e",
   "metadata": {},
   "source": [
    "## Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefbdce5-4287-4740-bdbd-729d15d8ab7f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This lab is recommended for students who have enrolled in the [**Building and deploying machine learning solutions with Vertex AI**](). Are you ready for the challenge?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8955d75d-cfa4-43af-8783-d2aec5ae525e",
   "metadata": {},
   "source": [
    "## Learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b386b37c-2ce1-4b1f-8c90-b83bda6075c8",
   "metadata": {},
   "source": [
    "* Train a TensorFlow model locally in a hosted [**Vertex Notebook**](https://cloud.google.com/vertex-ai/docs/general/notebooks?hl=sv).\n",
    "* Containerize your training code with [**Cloud Build**](https://cloud.google.com/build) and push it to [**Google Cloud Artifact Registry**](https://cloud.google.com/artifact-registry) as part of a Vertex custom container training workflow.\n",
    "* Deploy your trained model to a [**Vertex Online Prediction Endpoint**](https://cloud.google.com/vertex-ai/docs/predictions/getting-predictions) for serving predictions.\n",
    "* Request an online prediction and see the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d23538a-e809-4747-9bd4-5610f8544ea1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4480c8-710c-40dd-93c2-c51e67e59760",
   "metadata": {},
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0feaf4-9849-4636-b736-d3cd8a051579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add installed library dependencies to Python PATH variable.\n",
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68df5dd-c456-4edd-8f58-71597f10c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and set PROJECT_ID and REGION environment variables.\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3912f9-6c12-439f-8613-cc60c286b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a globally unique Google Cloud Storage bucket for artifact storage.\n",
    "GCS_BUCKET = f\"gs://{PROJECT_ID}-vertex-challenge-lab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4931ae91-3ba1-437a-9c37-187a41a3d227",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb -la $REGION $GCS_BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fbf656-73c9-41a4-aa61-5065b0d6a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = \"dougkelly\"  # <---CHANGE THIS\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/{}\".format(GCS_BUCKET, USER)\n",
    "\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ebbc2b-21ad-47f0-829f-9beba0deba9d",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf558fc-d0fc-4452-8281-7d7cd0cffe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import logging\n",
    "from typing import NamedTuple\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from google.cloud import aiplatform as vertexai\n",
    "\n",
    "import kfp\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (ClassificationMetrics, Input, Metrics, Model, Output,\n",
    "                        component)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d296167a-13b9-4895-be8b-b3b49fad5d47",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3a43371e-2c64-4a76-8698-fa768043dbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project=PROJECT_ID, location=REGION, staging_bucket=GCS_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef491df4-c35f-4555-a6b6-96114c3d3c6e",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eace2496-c64f-4448-a826-85e986621f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "\n",
    "dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n",
    "                                  untar=True, cache_dir='.',\n",
    "                                  cache_subdir='')\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "\n",
    "# remove unused folders to make it easier to load the data\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22775007-ded8-4a4a-ba71-c8f5126ce3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil -m cp -r {dataset_dir} {GCS_BUCKET}/data/acllmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50ee40c0-9e37-483c-98f5-dcdb467a2bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "class_names = raw_train_ds.class_names\n",
    "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/test',\n",
    "    batch_size=batch_size)\n",
    "\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d794068-817c-4cb8-8e4c-c49860d0c92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: b'\"Pandemonium\" is a horror movie spoof that comes off more stupid than funny. Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\\'t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\\'s all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)'\n",
      "Label : 0 (neg)\n",
      "Review: b\"David Mamet is a very interesting and a very un-equal director. His first movie 'House of Games' was the one I liked best, and it set a series of films with characters whose perspective of life changes as they get into complicated situations, and so does the perspective of the viewer.<br /><br />So is 'Homicide' which from the title tries to set the mind of the viewer to the usual crime drama. The principal characters are two cops, one Jewish and one Irish who deal with a racially charged area. The murder of an old Jewish shop owner who proves to be an ancient veteran of the Israeli Independence war triggers the Jewish identity in the mind and heart of the Jewish detective.<br /><br />This is were the flaws of the film are the more obvious. The process of awakening is theatrical and hard to believe, the group of Jewish militants is operatic, and the way the detective eventually walks to the final violent confrontation is pathetic. The end of the film itself is Mamet-like smart, but disappoints from a human emotional perspective.<br /><br />Joe Mantegna and William Macy give strong performances, but the flaws of the story are too evident to be easily compensated.\"\n",
      "Label : 0 (neg)\n",
      "Review: b'Great documentary about the lives of NY firefighters during the worst terrorist attack of all time.. That reason alone is why this should be a must see collectors item.. What shocked me was not only the attacks, but the\"High Fat Diet\" and physical appearance of some of these firefighters. I think a lot of Doctors would agree with me that,in the physical shape they were in, some of these firefighters would NOT of made it to the 79th floor carrying over 60 lbs of gear. Having said that i now have a greater respect for firefighters and i realize becoming a firefighter is a life altering job. The French have a history of making great documentary\\'s and that is what this is, a Great Documentary.....'\n",
      "Label : 1 (pos)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-07 16:41:36.577696: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  for i in range(3):\n",
    "    print(f'Review: {text_batch.numpy()[i]}')\n",
    "    label = label_batch.numpy()[i]\n",
    "    print(f'Label : {label} ({class_names[label]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3e6686-2fa1-453c-8259-8e5a87cba023",
   "metadata": {},
   "source": [
    "## Choose a BERT model to fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f825f49-2043-4ec4-b5e4-e16a855ecad2",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/text/tutorials/classify_text_with_bert#choose_a_bert_model_to_fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc39eec-72c6-4e27-933b-c2e7246e0b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TFHUB_BERT_PREPROCESSOR = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "TFHUB_BERT_ENCODER = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005b4646-ce95-47c4-b1f7-886f59980386",
   "metadata": {},
   "source": [
    "## Build and train a TensorFlow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289da96f-2aad-4c34-85ce-5916ea98778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text_classifier():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessor = hub.KerasLayer(TFHUB_BERT_PREPROCESSOR, name='preprocessing')\n",
    "    encoder_inputs = preprocessor(text_input)\n",
    "    encoder = hub.KerasLayer(TFHUB_BERT_ENCODER, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs[\"pooled_output\"]      # [batch_size, 512].\n",
    "    net = tf.keras.layers.Dropout(0.1, name=\"dropout\")(net)\n",
    "    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "    return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59cab23-fbf0-44d2-9ee0-e2dd83390fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test = ['this is such an amazing movie!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9f64fe-4672-4c22-be9c-fbfa33a351c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = build_text_classifier()\n",
    "bert_raw_result = classifier_model(tf.constant(text_test))\n",
    "print(bert_raw_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d9d70a-d921-4e4a-aeed-24dd093e8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(classifier_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ddd229-13a7-48a6-b3fd-b44beb780a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = tf.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1462f28d-95c0-4692-98b7-1fe7e226abb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a30a97-9229-4e3a-a81f-ded26aa6293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "classifier_model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba48201-3e19-4fe0-9747-c198f2f71cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Training model with {TFHUB_BERT_ENCODER}')\n",
    "history = classifier_model.fit(x=train_ds,\n",
    "                               validation_data=val_ds,\n",
    "                               epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d15a1fa-816b-4f0f-a54b-b5dd2547465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = classifier_model.evaluate(test_ds)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91980420-8451-4869-b189-2b3693131ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fc3fed-aa4c-40b2-8c44-19be21ba4689",
   "metadata": {},
   "source": [
    "## Containerize your model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0129184d-15ed-4ebe-bbdc-6c2687eb18cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bert-sentiment-classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "2594afe7-b9e0-4957-9156-d2e595fde62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bert-sentiment-classifier/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {MODEL_NAME}/trainer/model.py\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "import tensorflow_hub as hub\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "DATA_URL = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "LOCAL_DATA_DIR = './tmp/data'\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "def download_data(data_dir):\n",
    "    \"\"\"Download dataset.\"\"\"\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    dataset = tf.keras.utils.get_file(\n",
    "      fname='aclImdb_v1.tar.gz',\n",
    "      origin=DATA_URL,\n",
    "      untar=True,\n",
    "      cache_dir=data_dir,\n",
    "      cache_subdir=\"\")\n",
    "    \n",
    "    dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "    \n",
    "    train_dir = os.path.join(dataset_dir, 'train')\n",
    "    \n",
    "    # remove unused folders to make it easier to load the data\n",
    "    remove_dir = os.path.join(train_dir, 'unsup')\n",
    "    shutil.rmtree(remove_dir)\n",
    "    \n",
    "    return dataset_dir\n",
    "\n",
    "\n",
    "def load_datasets(dataset_dir, hparams):\n",
    "    \"\"\"Load pre-split tf.datasets.\n",
    "    Args:\n",
    "      hparams(dict): A dictionary containing model training arguments.\n",
    "    Returns:\n",
    "      raw_train_ds(tf.dataset):\n",
    "      raw_val_ds(tf.dataset):\n",
    "      raw_test_ds(tf.dataset):      \n",
    "    \"\"\"    \n",
    "\n",
    "    raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "        os.path.join(dataset_dir, 'train'),\n",
    "        batch_size=hparams['batch-size'],\n",
    "        validation_split=0.2,\n",
    "        subset='training',\n",
    "        seed=SEED)    \n",
    "\n",
    "    raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "        os.path.join(dataset_dir, 'train'),\n",
    "        batch_size=hparams['batch-size'],\n",
    "        validation_split=0.2,\n",
    "        subset='validation',\n",
    "        seed=SEED)\n",
    "\n",
    "    raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "        os.path.join(dataset_dir, 'test'),\n",
    "        batch_size=hparams['batch-size'])\n",
    "    \n",
    "    return raw_train_ds, raw_val_ds, raw_test_ds\n",
    "\n",
    "\n",
    "def build_text_classifier(hparams, optimizer):\n",
    "    \"\"\"Train and evaluate TensorFlow BERT sentiment classifier.\n",
    "    Args:\n",
    "      hparams(dict): A dictionary containing model training arguments.\n",
    "    Returns:\n",
    "      history(tf.keras.callbacks.History): Keras callback that records training event history.\n",
    "    \"\"\"\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessor = hub.KerasLayer(hparams['tf-hub-bert-preprocessor'], name='preprocessing')\n",
    "    encoder_inputs = preprocessor(text_input)\n",
    "    encoder = hub.KerasLayer(hparams['tf-hub-bert-encoder'], trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    classifier = outputs[\"pooled_output\"]\n",
    "    classifier = tf.keras.layers.Dropout(hparams['dropout'], name=\"dropout\")(classifier)\n",
    "    classifier = tf.keras.layers.Dense(1, activation=None, name='classifier')(classifier)\n",
    "    \n",
    "    model = tf.keras.Model(text_input, classifier)   \n",
    "    \n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    metrics = tf.metrics.BinaryAccuracy()    \n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss,\n",
    "                  metrics=metrics)    \n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_evaluate(hparams):\n",
    "    \"\"\"Train and evaluate TensorFlow BERT sentiment classifier.\n",
    "    Args:\n",
    "      hparams(dict): A dictionary containing model training arguments.\n",
    "    Returns:\n",
    "      history(tf.keras.callbacks.History): Keras callback that records training event history.\n",
    "    \"\"\"\n",
    "    dataset_dir = download_data(LOCAL_DATA_DIR)\n",
    "    train_ds, val_ds, test_ds = load_datasets(dataset_dir, hparams)\n",
    "    \n",
    "    train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)     \n",
    "    \n",
    "    epochs = hparams['epochs']\n",
    "    steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "    n_train_steps = steps_per_epoch * epochs\n",
    "    n_warmup_steps = int(0.1 * n_train_steps)    \n",
    "    \n",
    "    optimizer = optimization.create_optimizer(init_lr=hparams['initial-learning-rate'],\n",
    "                                              num_train_steps=n_train_steps,\n",
    "                                              num_warmup_steps=n_warmup_steps,\n",
    "                                              optimizer_type='adamw')    \n",
    "    \n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    with mirrored_strategy.scope():\n",
    "        model = build_text_classifier(hparams=hparams, optimizer=optimizer)\n",
    "        logging.info(model.summary())\n",
    "        \n",
    "    history = model.fit(x=train_ds,\n",
    "                        validation_data=val_ds,\n",
    "                        epochs=epochs)  \n",
    "    \n",
    "    logging.info(\"Test accuracy: %s\", model.evaluate(test_ds))\n",
    "\n",
    "    # Export Keras model in TensorFlow SavedModel format.\n",
    "    model.save(hparams['model-dir'])\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "17517e0b-a2ac-489a-bf03-357ace5d6577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bert-sentiment-classifier/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {MODEL_NAME}/trainer/task.py\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from trainer import model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Vertex custom container training args. These are set by Vertex AI during training but can also be overwritten.\n",
    "    parser.add_argument('--model-dir', dest='model-dir',\n",
    "                        default=os.environ['AIP_MODEL_DIR'], type=str, help='GCS URI for saving model artifacts.')\n",
    "    # parser.add_argument('--data-dir', dest='data-dir',\n",
    "    #                     default='gs://dougkelly-vertex-demos-vertex-challenge-lab/data/acllmdb', type=str, help='GCS URI for saving dataset.') \n",
    "    parser.add_argument('--tf-hub-bert-preprocessor', dest='tf-hub-bert-preprocessor', \n",
    "                        default='https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3', type=str, help='TF-Hub URL.')\n",
    "    parser.add_argument('--tf-hub-bert-encoder', dest='tf-hub-bert-encoder', \n",
    "                        default='https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2', type=str, help='TF-Hub URL.')     \n",
    "\n",
    "    # Model training args.\n",
    "    parser.add_argument('--initial-learning-rate', dest='initial-learning-rate', default=3e-5, type=float, help='Learning rate for optimizer.')\n",
    "    parser.add_argument('--epochs', dest='epochs', default=5, type=int, help='Training iterations.')    \n",
    "    parser.add_argument('--batch-size', dest='batch-size', default=32, type=int, help='Number of examples during each training iteration.')    \n",
    "    parser.add_argument('--dropout', dest='dropout', default=0.1, type=float, help='Float percentage of DNN nodes [0,1] to drop for regularization.')    \n",
    "\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    hparams = args.__dict__\n",
    "\n",
    "    model.train_evaluate(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d503a04a-5f15-4503-91e9-1acc4353fd08",
   "metadata": {},
   "source": [
    "### Write a `Dockerfile` for your custom model container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "b86ede10-6372-4320-89d3-264d4d1b1ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bert-sentiment-classifier/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {MODEL_NAME}/Dockerfile\n",
    "# Specifies base image and tag.\n",
    "# https://cloud.google.com/vertex-ai/docs/general/deep-learning\n",
    "# https://cloud.google.com/deep-learning-containers/docs/choosing-container\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "\n",
    "# Sets the container working directory.\n",
    "WORKDIR /root\n",
    "\n",
    "# Copies the requirements.txt into the container to reduce network calls.\n",
    "COPY requirements.txt .\n",
    "# Installs additional packages.\n",
    "RUN pip3 install -U -r requirements.txt\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY . /trainer\n",
    "\n",
    "# Sets the container working directory.\n",
    "WORKDIR /trainer\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "fe7619e1-fff9-4a47-90a8-3ba9b55e74c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bert-sentiment-classifier/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {MODEL_NAME}/requirements.txt\n",
    "tensorflow==2.5\n",
    "tensorflow-text==2.5.0\n",
    "tf-models-official==2.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81292584-7a08-4c92-a9ba-e3dbc7005130",
   "metadata": {},
   "source": [
    "## Use Cloud Build to build and submit your container to Google Cloud Artifact Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860c0d02-200f-4cc3-bdfd-ba96d233ecc4",
   "metadata": {},
   "source": [
    "### Create Artifact Repository for custom container images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f9918475-f6dc-4fa3-8249-47976e68f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACT_REPOSITORY=\"bert-sentiment-classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "93d9566d-c6c4-48c8-b4f8-ad239e5ae349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n"
     ]
    }
   ],
   "source": [
    "# Create an Artifact Repository using the gcloud CLI.\n",
    "!gcloud artifacts repositories create $ARTIFACT_REPOSITORY \\\n",
    "--repository-format=docker \\\n",
    "--location=$REGION \\\n",
    "--description=\"Artifact registry for ML custom training images for sentiment classification\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e900832e-de90-4ba1-ba7d-7973a1de9cc1",
   "metadata": {},
   "source": [
    "### Create `cloudbuild.yaml` instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "b580619d-957c-409f-ac15-ccbc0bb79a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME=\"bert-sentiment-classifier\"\n",
    "IMAGE_TAG=\"latest\"\n",
    "IMAGE_URI=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{ARTIFACT_REPOSITORY}/{IMAGE_NAME}:{IMAGE_TAG}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "24790970-988e-4694-b8cd-a2d9d500c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudbuild_yaml = f\"\"\"steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: [ 'build', '-t', '{IMAGE_URI}', '.' ]\n",
    "images: \n",
    "- '{IMAGE_URI}'\"\"\"\n",
    "\n",
    "with open(f\"{MODEL_NAME}/cloudbuild.yaml\", \"w\") as fp:\n",
    "    fp.write(cloudbuild_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c14b9c6-c120-482d-b5ab-c6a4c53bb205",
   "metadata": {},
   "source": [
    "### Build and submit your container image to your Artifact Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "3ebf0093-d66e-49c1-8a55-047020735e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 7 file(s) totalling 7.3 KiB before compression.\n",
      "Uploading tarball of [bert-sentiment-classifier] to [gs://dougkelly-vertex-demos_cloudbuild/source/1633670786.325722-abf2ada3f3024d2cae7d48c11ed5c1be.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/dougkelly-vertex-demos/locations/global/builds/dbf22e46-a66c-498f-b505-e21806846eec].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/dbf22e46-a66c-498f-b505-e21806846eec?project=617979904441].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"dbf22e46-a66c-498f-b505-e21806846eec\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://dougkelly-vertex-demos_cloudbuild/source/1633670786.325722-abf2ada3f3024d2cae7d48c11ed5c1be.tgz#1633670786695816\n",
      "Copying gs://dougkelly-vertex-demos_cloudbuild/source/1633670786.325722-abf2ada3f3024d2cae7d48c11ed5c1be.tgz#1633670786695816...\n",
      "/ [1 files][  2.8 KiB/  2.8 KiB]                                                \n",
      "Operation completed over 1 objects/2.8 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  15.36kB\n",
      "Step 1/7 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "e4ca327ec0e7: Pulling fs layer\n",
      "d92f1b6ebc96: Pulling fs layer\n",
      "caf909099ad7: Pulling fs layer\n",
      "f2436a8b630b: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "2142b3064d0c: Pulling fs layer\n",
      "8a5701c6f2ce: Pulling fs layer\n",
      "3f8d66c569d3: Pulling fs layer\n",
      "09401fb91b32: Pulling fs layer\n",
      "99f814694728: Pulling fs layer\n",
      "ce0dab248284: Pulling fs layer\n",
      "3e47e6559d28: Pulling fs layer\n",
      "351fc3c444d6: Pulling fs layer\n",
      "12b1e84e616f: Pulling fs layer\n",
      "0fa9d14c1b75: Pulling fs layer\n",
      "03db6fc56e5d: Pulling fs layer\n",
      "720e2a0d5c72: Pulling fs layer\n",
      "6f6d5ad73901: Pulling fs layer\n",
      "4f4fb700ef54: Waiting\n",
      "2142b3064d0c: Waiting\n",
      "8a5701c6f2ce: Waiting\n",
      "3f8d66c569d3: Waiting\n",
      "09401fb91b32: Waiting\n",
      "99f814694728: Waiting\n",
      "ce0dab248284: Waiting\n",
      "3e47e6559d28: Waiting\n",
      "351fc3c444d6: Waiting\n",
      "12b1e84e616f: Waiting\n",
      "0fa9d14c1b75: Waiting\n",
      "03db6fc56e5d: Waiting\n",
      "720e2a0d5c72: Waiting\n",
      "6f6d5ad73901: Waiting\n",
      "f2436a8b630b: Waiting\n",
      "d92f1b6ebc96: Verifying Checksum\n",
      "d92f1b6ebc96: Download complete\n",
      "e4ca327ec0e7: Verifying Checksum\n",
      "e4ca327ec0e7: Download complete\n",
      "4f4fb700ef54: Download complete\n",
      "2142b3064d0c: Verifying Checksum\n",
      "2142b3064d0c: Download complete\n",
      "f2436a8b630b: Verifying Checksum\n",
      "f2436a8b630b: Download complete\n",
      "3f8d66c569d3: Verifying Checksum\n",
      "3f8d66c569d3: Download complete\n",
      "09401fb91b32: Verifying Checksum\n",
      "09401fb91b32: Download complete\n",
      "99f814694728: Verifying Checksum\n",
      "99f814694728: Download complete\n",
      "ce0dab248284: Verifying Checksum\n",
      "ce0dab248284: Download complete\n",
      "8a5701c6f2ce: Verifying Checksum\n",
      "8a5701c6f2ce: Download complete\n",
      "3e47e6559d28: Verifying Checksum\n",
      "3e47e6559d28: Download complete\n",
      "12b1e84e616f: Verifying Checksum\n",
      "12b1e84e616f: Download complete\n",
      "351fc3c444d6: Verifying Checksum\n",
      "351fc3c444d6: Download complete\n",
      "0fa9d14c1b75: Verifying Checksum\n",
      "0fa9d14c1b75: Download complete\n",
      "03db6fc56e5d: Verifying Checksum\n",
      "03db6fc56e5d: Download complete\n",
      "6f6d5ad73901: Verifying Checksum\n",
      "6f6d5ad73901: Download complete\n",
      "caf909099ad7: Verifying Checksum\n",
      "caf909099ad7: Download complete\n",
      "e4ca327ec0e7: Pull complete\n",
      "d92f1b6ebc96: Pull complete\n",
      "720e2a0d5c72: Verifying Checksum\n",
      "720e2a0d5c72: Download complete\n",
      "caf909099ad7: Pull complete\n",
      "f2436a8b630b: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "2142b3064d0c: Pull complete\n",
      "8a5701c6f2ce: Pull complete\n",
      "3f8d66c569d3: Pull complete\n",
      "09401fb91b32: Pull complete\n",
      "99f814694728: Pull complete\n",
      "ce0dab248284: Pull complete\n",
      "3e47e6559d28: Pull complete\n",
      "351fc3c444d6: Pull complete\n",
      "12b1e84e616f: Pull complete\n",
      "0fa9d14c1b75: Pull complete\n",
      "03db6fc56e5d: Pull complete\n",
      "720e2a0d5c72: Pull complete\n",
      "6f6d5ad73901: Pull complete\n",
      "Digest: sha256:f30d5a97e461415a3f30e9b9354e72c5a09b9854f676cf8a39c2f77bc1ee6950\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> 6b1264b8c37c\n",
      "Step 2/7 : WORKDIR /root\n",
      " ---> Running in a59ecc7773fa\n",
      "Removing intermediate container a59ecc7773fa\n",
      " ---> 1999bce38ff1\n",
      "Step 3/7 : COPY requirements.txt .\n",
      " ---> 5274f27b86d2\n",
      "Step 4/7 : RUN pip3 install -U -r requirements.txt\n",
      " ---> Running in 10e5f6460c64\n",
      "Collecting tensorflow==2.5\n",
      "  Downloading tensorflow-2.5.0-cp37-cp37m-manylinux2010_x86_64.whl (454.3 MB)\n",
      "Collecting tensorflow-text==2.5.0\n",
      "  Downloading tensorflow_text-2.5.0-cp37-cp37m-manylinux1_x86_64.whl (4.3 MB)\n",
      "Collecting tf-models-official==2.5.0\n",
      "  Downloading tf_models_official-2.5.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.5->-r requirements.txt (line 1)) (1.12.1)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.5->-r requirements.txt (line 1)) (1.19.5)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.14.1-py3-none-any.whl (131 kB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting keras-nightly~=2.5.0.dev\n",
      "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting tensorboard~=2.5\n",
      "  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.5->-r requirements.txt (line 1)) (3.18.0)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
      "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting grpcio~=1.34.0\n",
      "  Downloading grpcio-1.34.1-cp37-cp37m-manylinux2014_x86_64.whl (4.0 MB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting six~=1.15.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.5->-r requirements.txt (line 1)) (0.37.0)\n",
      "Collecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n",
      "Collecting tensorflow-hub>=0.8.0\n",
      "  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "Collecting kaggle>=1.3.9\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "Collecting pycocotools\n",
      "  Downloading pycocotools-2.0.2.tar.gz (23 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0->-r requirements.txt (line 3)) (5.4.1)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0->-r requirements.txt (line 3)) (1.7.1)\n",
      "Collecting opencv-python-headless\n",
      "  Downloading opencv_python_headless-4.5.3.56-cp37-cp37m-manylinux2014_x86_64.whl (37.1 MB)\n",
      "Collecting gin-config\n",
      "  Downloading gin_config-0.4.0-py2.py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: psutil>=5.4.3 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0->-r requirements.txt (line 3)) (5.8.0)\n",
      "Collecting tf-slim>=1.1.0\n",
      "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0->-r requirements.txt (line 3)) (1.3.3)\n",
      "Collecting oauth2client\n",
      "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "Collecting py-cpuinfo>=3.3.0\n",
      "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0->-r requirements.txt (line 3)) (3.4.3)\n",
      "Collecting Cython\n",
      "  Using cached Cython-0.29.24-cp37-cp37m-manylinux1_x86_64.whl (2.0 MB)\n",
      "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0->-r requirements.txt (line 3)) (2.26.0)\n",
      "Collecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.14.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "Requirement already satisfied: google-api-python-client>=1.6.7 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0->-r requirements.txt (line 3)) (2.22.0)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0->-r requirements.txt (line 3)) (8.3.2)\n",
      "Collecting tensorflow-model-optimization>=0.4.1\n",
      "  Downloading tensorflow_model_optimization-0.7.0-py2.py3-none-any.whl (213 kB)\n",
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.4.0-py3-none-any.whl (4.0 MB)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0->-r requirements.txt (line 3)) (0.19.1)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0->-r requirements.txt (line 3)) (1.35.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0->-r requirements.txt (line 3)) (0.1.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0->-r requirements.txt (line 3)) (3.0.1)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0->-r requirements.txt (line 3)) (1.31.2)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0->-r requirements.txt (line 3)) (2021.1)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0->-r requirements.txt (line 3)) (58.0.4)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0->-r requirements.txt (line 3)) (21.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0->-r requirements.txt (line 3)) (2.25.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0->-r requirements.txt (line 3)) (1.53.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0->-r requirements.txt (line 3)) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0->-r requirements.txt (line 3)) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0->-r requirements.txt (line 3)) (0.2.7)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0->-r requirements.txt (line 3)) (2.0.3)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0->-r requirements.txt (line 3)) (2.0.0)\n",
      "Requirement already satisfied: proto-plus>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0->-r requirements.txt (line 3)) (1.19.0)\n",
      "Collecting google-cloud-bigquery>=0.31.0\n",
      "  Downloading google_cloud_bigquery-2.28.1-py2.py3-none-any.whl (202 kB)\n",
      "  Downloading google_cloud_bigquery-2.28.0-py2.py3-none-any.whl (202 kB)\n",
      "  Downloading google_cloud_bigquery-2.27.1-py2.py3-none-any.whl (201 kB)\n",
      "  Downloading google_cloud_bigquery-2.27.0-py2.py3-none-any.whl (201 kB)\n",
      "  Downloading google_cloud_bigquery-2.25.2-py2.py3-none-any.whl (200 kB)\n",
      "  Downloading google_cloud_bigquery-2.25.1-py2.py3-none-any.whl (200 kB)\n",
      "  Downloading google_cloud_bigquery-2.25.0-py2.py3-none-any.whl (200 kB)\n",
      "  Downloading google_cloud_bigquery-2.24.1-py2.py3-none-any.whl (198 kB)\n",
      "  Downloading google_cloud_bigquery-2.24.0-py2.py3-none-any.whl (198 kB)\n",
      "  Downloading google_cloud_bigquery-2.23.3-py2.py3-none-any.whl (196 kB)\n",
      "  Downloading google_cloud_bigquery-2.23.2-py2.py3-none-any.whl (196 kB)\n",
      "  Downloading google_cloud_bigquery-2.23.1-py2.py3-none-any.whl (196 kB)\n",
      "  Downloading google_cloud_bigquery-2.23.0-py2.py3-none-any.whl (196 kB)\n",
      "  Downloading google_cloud_bigquery-2.22.1-py2.py3-none-any.whl (195 kB)\n",
      "  Downloading google_cloud_bigquery-2.22.0-py2.py3-none-any.whl (194 kB)\n",
      "  Downloading google_cloud_bigquery-2.21.0-py2.py3-none-any.whl (193 kB)\n",
      "  Downloading google_cloud_bigquery-2.20.0-py2.py3-none-any.whl (189 kB)\n",
      "Collecting google-cloud-core<2.0dev,>=1.4.1\n",
      "  Downloading google_cloud_core-1.7.2-py2.py3-none-any.whl (28 kB)\n",
      "Collecting google-resumable-media<2.0dev,>=0.6.0\n",
      "  Downloading google_resumable_media-1.3.3-py2.py3-none-any.whl (75 kB)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0->-r requirements.txt (line 3)) (1.2.0)\n",
      "Collecting cached-property\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0->-r requirements.txt (line 3)) (2.4.7)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0->-r requirements.txt (line 3)) (2021.5.30)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0->-r requirements.txt (line 3)) (4.62.3)\n",
      "Requirement already satisfied: python-slugify in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0->-r requirements.txt (line 3)) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0->-r requirements.txt (line 3)) (1.26.6)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0->-r requirements.txt (line 3)) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3.0.0dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0->-r requirements.txt (line 3)) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3.0.0dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0->-r requirements.txt (line 3)) (4.0.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow==2.5->-r requirements.txt (line 1)) (0.4.6)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.2-py3-none-any.whl (288 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow==2.5->-r requirements.txt (line 1)) (3.3.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5->-r requirements.txt (line 1)) (4.8.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5->-r requirements.txt (line 1)) (3.1.1)\n",
      "Collecting dm-tree~=0.1.1\n",
      "  Downloading dm_tree-0.1.6-cp37-cp37m-manylinux_2_24_x86_64.whl (93 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->tf-models-official==2.5.0->-r requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->tf-models-official==2.5.0->-r requirements.txt (line 3)) (0.10.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.7/site-packages (from python-slugify->kaggle>=1.3.9->tf-models-official==2.5.0->-r requirements.txt (line 3)) (1.3)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from sacrebleu->tf-models-official==2.5.0->-r requirements.txt (line 3)) (0.4.4)\n",
      "Collecting tabulate>=0.8.9\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from sacrebleu->tf-models-official==2.5.0->-r requirements.txt (line 3)) (2021.8.28)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval->tf-models-official==2.5.0->-r requirements.txt (line 3)) (0.24.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.5.0->-r requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.5.0->-r requirements.txt (line 3)) (2.2.0)\n",
      "Collecting typeguard>=2.7\n",
      "  Downloading typeguard-2.12.1-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official==2.5.0->-r requirements.txt (line 3)) (0.18.2)\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-5.2.2-py3-none-any.whl (27 kB)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official==2.5.0->-r requirements.txt (line 3)) (21.2.0)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.2.0-py3-none-any.whl (48 kB)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "Building wheels for collected packages: kaggle, py-cpuinfo, termcolor, pycocotools, seqeval, promise\n",
      "  Building wheel for kaggle (setup.py): started\n",
      "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=fb63e1a5610e4acd3cb3c2280cc8b75cea3654940d57aa988cc63390baaa885f\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
      "  Building wheel for py-cpuinfo (setup.py): started\n",
      "  Building wheel for py-cpuinfo (setup.py): finished with status 'done'\n",
      "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22258 sha256=5075c97b5ee26952d7868dcac7a26f2e387506b593e84ea5ffbc28df1bf0dd05\n",
      "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=8f276aa7254b3ce4254a817fb8cf8f24a81ad7dec0da85f5a8e17a8e09e6bdc8\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "  Building wheel for pycocotools (setup.py): started\n",
      "  Building wheel for pycocotools (setup.py): finished with status 'done'\n",
      "  Created wheel for pycocotools: filename=pycocotools-2.0.2-cp37-cp37m-linux_x86_64.whl size=272251 sha256=2065845666a02f8e2e20c6f42932591e021a8bec214017234bb1b1354c0a90a8\n",
      "  Stored in directory: /root/.cache/pip/wheels/bc/cf/1b/e95c99c5f9d1648be3f500ca55e7ce55f24818b0f48336adaf\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=dfeda5d687794fb3db751468efa40889f93ae02532995f42439b2b9131c33474\n",
      "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21502 sha256=e9f6a820c2963463e8f406b64a4d18b070b8e184b93c35fa4f04c71688a42780\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/93/c6/762e359f8cb6a5b69c72235d798804cae523bbe41c2aa8333d\n",
      "Successfully built kaggle py-cpuinfo termcolor pycocotools seqeval promise\n",
      "Installing collected packages: typing-extensions, six, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, grpcio, cached-property, absl-py, typeguard, termcolor, tensorflow-metadata, tensorflow-estimator, tensorboard, tabulate, promise, portalocker, opt-einsum, keras-preprocessing, keras-nightly, importlib-resources, h5py, google-resumable-media, google-pasta, google-cloud-core, gast, flatbuffers, dm-tree, dill, Cython, astunparse, tf-slim, tensorflow-model-optimization, tensorflow-hub, tensorflow-datasets, tensorflow-addons, tensorflow, seqeval, sentencepiece, sacrebleu, pycocotools, py-cpuinfo, opencv-python-headless, oauth2client, kaggle, google-cloud-bigquery, gin-config, tf-models-official, tensorflow-text\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.2\n",
      "    Uninstalling typing-extensions-3.10.0.2:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.2\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.38.1\n",
      "    Uninstalling grpcio-1.38.1:\n",
      "      Successfully uninstalled grpcio-1.38.1\n",
      "  Attempting uninstall: google-resumable-media\n",
      "    Found existing installation: google-resumable-media 2.0.3\n",
      "    Uninstalling google-resumable-media-2.0.3:\n",
      "      Successfully uninstalled google-resumable-media-2.0.3\n",
      "  Attempting uninstall: google-cloud-core\n",
      "    Found existing installation: google-cloud-core 2.0.0\n",
      "    Uninstalling google-cloud-core-2.0.0:\n",
      "      Successfully uninstalled google-cloud-core-2.0.0\n",
      "  Attempting uninstall: google-cloud-bigquery\n",
      "    Found existing installation: google-cloud-bigquery 2.26.0\n",
      "    Uninstalling google-cloud-bigquery-2.26.0:\n",
      "      Successfully uninstalled google-cloud-bigquery-2.26.0\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "black 21.9b0 requires typing-extensions>=3.10.0.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n",
      "\u001b[0mSuccessfully installed Cython-0.29.24 absl-py-0.12.0 astunparse-1.6.3 cached-property-1.5.2 dill-0.3.4 dm-tree-0.1.6 flatbuffers-1.12 gast-0.4.0 gin-config-0.4.0 google-cloud-bigquery-2.20.0 google-cloud-core-1.7.2 google-pasta-0.2.0 google-resumable-media-1.3.3 grpcio-1.34.1 h5py-3.1.0 importlib-resources-5.2.2 kaggle-1.5.12 keras-nightly-2.5.0.dev2021032900 keras-preprocessing-1.1.2 oauth2client-4.1.3 opencv-python-headless-4.5.3.56 opt-einsum-3.3.0 portalocker-2.3.2 promise-2.3 py-cpuinfo-8.0.0 pycocotools-2.0.2 sacrebleu-2.0.0 sentencepiece-0.1.96 seqeval-1.2.2 six-1.15.0 tabulate-0.8.9 tensorboard-2.6.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.5.0 tensorflow-addons-0.14.0 tensorflow-datasets-4.4.0 tensorflow-estimator-2.5.0 tensorflow-hub-0.12.0 tensorflow-metadata-1.2.0 tensorflow-model-optimization-0.7.0 tensorflow-text-2.5.0 termcolor-1.1.0 tf-models-official-2.5.0 tf-slim-1.1.0 typeguard-2.12.1 typing-extensions-3.7.4.3 werkzeug-2.0.2\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 10e5f6460c64\n",
      " ---> a3027ede679f\n",
      "Step 5/7 : COPY . /trainer\n",
      " ---> 9da4a7e7e80f\n",
      "Step 6/7 : WORKDIR /trainer\n",
      " ---> Running in 8a18448e9b0c\n",
      "Removing intermediate container 8a18448e9b0c\n",
      " ---> 32aca83df6ed\n",
      "Step 7/7 : ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]\n",
      " ---> Running in 94b4d128b4d2\n",
      "Removing intermediate container 94b4d128b4d2\n",
      " ---> 84db7d4a2d4d\n",
      "Successfully built 84db7d4a2d4d\n",
      "Successfully tagged us-central1-docker.pkg.dev/dougkelly-vertex-demos/bert-sentiment-classifier/bert-sentiment-classifier:latest\n",
      "PUSH\n",
      "Pushing us-central1-docker.pkg.dev/dougkelly-vertex-demos/bert-sentiment-classifier/bert-sentiment-classifier:latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/dougkelly-vertex-demos/bert-sentiment-classifier/bert-sentiment-classifier]\n",
      "76008a3c1d8c: Preparing\n",
      "ca08160317bd: Preparing\n",
      "1d0f18e4951b: Preparing\n",
      "8d8b6ee54d43: Preparing\n",
      "79eebddecd7b: Preparing\n",
      "d2f4b42995cf: Preparing\n",
      "fb81e5699636: Preparing\n",
      "4930ebb0a931: Preparing\n",
      "131ce3bb5d74: Preparing\n",
      "7659c32f59e4: Preparing\n",
      "85614aa64610: Preparing\n",
      "6f3c3f5b29db: Preparing\n",
      "b067ab39d28b: Preparing\n",
      "8ae6e3198a0c: Preparing\n",
      "5c2ad55d2779: Preparing\n",
      "675644f7a682: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "f22a37fa3c8c: Preparing\n",
      "60bb9a4914af: Preparing\n",
      "a68a3162c7c3: Preparing\n",
      "6babb56be259: Preparing\n",
      "d2f4b42995cf: Waiting\n",
      "fb81e5699636: Waiting\n",
      "4930ebb0a931: Waiting\n",
      "131ce3bb5d74: Waiting\n",
      "7659c32f59e4: Waiting\n",
      "85614aa64610: Waiting\n",
      "6f3c3f5b29db: Waiting\n",
      "b067ab39d28b: Waiting\n",
      "8ae6e3198a0c: Waiting\n",
      "675644f7a682: Waiting\n",
      "5c2ad55d2779: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "f22a37fa3c8c: Waiting\n",
      "60bb9a4914af: Waiting\n",
      "a68a3162c7c3: Waiting\n",
      "6babb56be259: Waiting\n",
      "8d8b6ee54d43: Layer already exists\n",
      "79eebddecd7b: Layer already exists\n",
      "d2f4b42995cf: Layer already exists\n",
      "fb81e5699636: Layer already exists\n",
      "4930ebb0a931: Layer already exists\n",
      "131ce3bb5d74: Layer already exists\n",
      "85614aa64610: Layer already exists\n",
      "7659c32f59e4: Layer already exists\n",
      "b067ab39d28b: Layer already exists\n",
      "76008a3c1d8c: Pushed\n",
      "6f3c3f5b29db: Layer already exists\n",
      "675644f7a682: Layer already exists\n",
      "8ae6e3198a0c: Layer already exists\n",
      "1d0f18e4951b: Pushed\n",
      "5c2ad55d2779: Layer already exists\n",
      "f22a37fa3c8c: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "60bb9a4914af: Layer already exists\n",
      "a68a3162c7c3: Layer already exists\n",
      "6babb56be259: Layer already exists\n",
      "ca08160317bd: Pushed\n",
      "latest: digest: sha256:a675e5ab0c319c4ff27a50e45bbaa399ee711c39935bd048f5cde4e44b421f92 size: 4709\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                IMAGES                                                                                                           STATUS\n",
      "dbf22e46-a66c-498f-b505-e21806846eec  2021-10-08T05:26:27+00:00  7M8S      gs://dougkelly-vertex-demos_cloudbuild/source/1633670786.325722-abf2ada3f3024d2cae7d48c11ed5c1be.tgz  us-central1-docker.pkg.dev/dougkelly-vertex-demos/bert-sentiment-classifier/bert-sentiment-classifier (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout=20m --config {MODEL_NAME}/cloudbuild.yaml {MODEL_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "bb766688-74dc-4583-a3a9-d831fac37819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-sentiment-20211008165901\n",
      "gs://dougkelly-vertex-demos-vertex-challenge-lab/bert-sentiment-classifier-20211008165901\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "TIMESTAMP=datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "DISPLAY_NAME = \"bert-sentiment-{}\".format(TIMESTAMP)\n",
    "GCS_BASE_OUTPUT_DIR= f\"{GCS_BUCKET}/{MODEL_NAME}-{TIMESTAMP}\"\n",
    "\n",
    "print(DISPLAY_NAME)\n",
    "print(GCS_BASE_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "6184e411-18fc-4e89-8bd0-771fd297cb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_IMAGE_URI = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-5:latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "344948a2-b36c-45f5-8bb4-abc2c5f9a7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# command line args for trainer.task defined above. Review the 'help' argument for a description.\n",
    "# You will set the model training args below. Vertex AI will set the environment variables for training URIs.\n",
    "# CMD_ARGS= [\n",
    "#     \"--learning-rate=\" + str(0.001),\n",
    "#     \"--batch-size=\" + str(16),\n",
    "#     \"--n-train-examples=\" + str(2638),\n",
    "#     \"--stop-point=\" + str(10),\n",
    "#     \"--n-checkpoints=\" + str(10),\n",
    "#     \"--dropout=\" + str(0.2),   \n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26348ae-7357-4a8f-8ad0-7c7f9347c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = vertexai.CustomContainerTrainingJob(\n",
    "    display_name=MODEL_NAME,\n",
    "    container_uri=IMAGE_URI,\n",
    "    model_serving_container_image_uri=SERVING_IMAGE_URI,\n",
    "    # https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers\n",
    "    # model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-3:latest\",\n",
    ")\n",
    "\n",
    "model = job.run(\n",
    "    # dataset=tabular_dataset,\n",
    "    # model_display_name=DISPLAY_NAME,\n",
    "    # GCS custom job output dir.\n",
    "    base_output_dir=GCS_BASE_OUTPUT_DIR,\n",
    "    # args=CMD_ARGS,\n",
    "    # Custom job WorkerPool arguments.\n",
    "    replica_count=1,\n",
    "    machine_type=\"c2-standard-4\",\n",
    "    # Provide your Tensorboard resource name to write Tensorboard logs during training.\n",
    "    # tensorboard=TENSORBOARD_RESOURCE_NAME,\n",
    "    # Provide your Vertex custom training service account created during lab setup.\n",
    "    # service_account=f\"vertex-custom-training-sa@{PROJECT_ID}.iam.gserviceaccount.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee35ac-ab83-472d-ab18-f622f3e3bc31",
   "metadata": {},
   "source": [
    "## Define a pipeline using the KFP SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "d2181f3d-10cd-49c8-8e2f-e5c314940321",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"bert-sentiment-classification\", pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(\n",
    "    project: str = PROJECT_ID,\n",
    "    gcp_region: str = REGION,\n",
    "    model_container_uri: str = IMAGE_URI,\n",
    "    model_serving_container_uri: str = SERVING_IMAGE_URI,\n",
    "    # cmd_args: list = CMD_ARGS,\n",
    "    staging_bucket: str = GCS_BUCKET,\n",
    "    gcs_base_output_dir: str = GCS_BASE_OUTPUT_DIR,\n",
    "    model_display_name: str = DISPLAY_NAME,\n",
    "):\n",
    "    \n",
    "    model_train_evaluate_op = gcc_aip.CustomContainerTrainingJobRunOp(\n",
    "        display_name=model_display_name,\n",
    "        container_uri=model_container_uri,\n",
    "        model_serving_container_image_uri=model_serving_container_uri,\n",
    "        # args=cmd_args,\n",
    "        staging_bucket=staging_bucket,\n",
    "        base_output_dir=gcs_base_output_dir,\n",
    "        # Custom job WorkerPool arguments.\n",
    "        replica_count=1,\n",
    "        machine_type=\"c2-standard-4\",        \n",
    "    )\n",
    "    \n",
    "    model_deploy_op = gcc_aip.ModelDeployOp(\n",
    "        model=model_train_evaluate_op.outputs[\"model\"],\n",
    "        project=project,\n",
    "        location=gcp_region,\n",
    "        machine_type=\"n1-standard-4\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783114fd-731b-4bad-bbe2-7a858e621fca",
   "metadata": {},
   "source": [
    "## Compile the pipeline using the KFP SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "eb28dac2-3721-4fe6-9e01-98745b0d1aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "77355b83-577b-4831-9862-91e08e974256",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"bert-sentiment-classification.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793cda30-4046-4d29-abdd-501c243f5eee",
   "metadata": {},
   "source": [
    "## Run the pipeline on Vertex Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "f276575d-c2ba-4d08-9a2a-b7583af27aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_pipelines_job = vertexai.pipeline_jobs.PipelineJob(\n",
    "    display_name=\"bert-sentiment-classification\",\n",
    "    template_path=\"bert-sentiment-classification.json\",\n",
    "    parameter_values={\"project\": PROJECT_ID,\n",
    "                      \"gcp_region\": REGION,\n",
    "                      \"model_container_uri\": IMAGE_URI,\n",
    "                      \"model_serving_container_uri\": SERVING_IMAGE_URI,\n",
    "                      \"staging_bucket\": GCS_BUCKET, \n",
    "                      \"gcs_base_output_dir\": GCS_BASE_OUTPUT_DIR,\n",
    "                      \"model_display_name\": DISPLAY_NAME},\n",
    "    enable_caching=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "f0ab35e9-207c-49ea-8a27-6e6cddce8541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/617979904441/locations/us-central1/pipelineJobs/bert-sentiment-classification-20211008170707\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/617979904441/locations/us-central1/pipelineJobs/bert-sentiment-classification-20211008170707')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/bert-sentiment-classification-20211008170707?project=617979904441\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/617979904441/locations/us-central1/pipelineJobs/bert-sentiment-classification-20211008170707 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/617979904441/locations/us-central1/pipelineJobs/bert-sentiment-classification-20211008170707 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/617979904441/locations/us-central1/pipelineJobs/bert-sentiment-classification-20211008170707 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [customcontainertrainingjob-run].; Job (project_id = dougkelly-vertex-demos, job_id = 3339040891695267840) is failed due to the above error.; Failed to handle the job: {project_number = 617979904441, job_id = 3339040891695267840}\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3163/565911317.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvertex_pipelines_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    667\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, service_account, network, sync)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"View Pipeline Job:\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dashboard_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36m_block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;31m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_PIPELINE_ERROR_STATES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Job failed with:\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_action_completed_against_resource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [customcontainertrainingjob-run].; Job (project_id = dougkelly-vertex-demos, job_id = 3339040891695267840) is failed due to the above error.; Failed to handle the job: {project_number = 617979904441, job_id = 3339040891695267840}\"\n"
     ]
    }
   ],
   "source": [
    "vertex_pipelines_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319a821a-a3bd-45bf-a9ea-aa18687218f6",
   "metadata": {},
   "source": [
    "## Query deployed model on Vertex Endpoint for online predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf80748b-8907-4ad6-8adb-d4c394752257",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-6.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m80"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
